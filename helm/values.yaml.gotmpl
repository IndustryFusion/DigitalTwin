mainVersion: {{ .StateValues.mainVersion }}
mainRepo: {{ .StateValues.mainRepo }}
namespace: {{ .StateValues.namespace }}
pullSecretCredentials: {{ .StateValues.pullSecretCredentials }}

ingressType: {{ .StateValues.ingressType }}
dbpassword: {{ .StateValues.dbpassword }}
oispdbPassword: {{ .StateValues.oispdbPassword }}
clusterSvcName: {{ .StateValues.db.teamId -}}-{{- .StateValues.db.clusterSvcPostfix }}
clusterExternalSvcName: {{ .StateValues.db.teamId -}}-{{- .StateValues.db.clusterSvcPostfix }}.{{ .StateValues.namespace }}.svc.cluster.local
minioAdminSecretKey: {{ .StateValues.minioAdminSecretKey }}
minioUserSecretKey: {{ .StateValues.minioUserSecretKey }}
alertaClientSecret: {{ .StateValues.alertaClientSecret }}


db:
  teamId: {{ .StateValues.db.teamId }}
  clusterSvcPostfix: {{ .StateValues.db.clusterSvcPostfix }}
  svcPort: {{ .StateValues.db.svcPort }}
  pvSize: {{ .StateValues.db.pvSize }}
  podInstances: {{ .StateValues.db.podInstances }}
  alertaDb: monitoring
  dbUser: {{ .StateValues.db.dbUser }}
  oispdbUser: {{ .StateValues.db.oispdbUser }}
  scorpioDb: ngb
  secretPostfix: credentials.postgresql.acid.zalan.do
  backupBucket: {{ .StateValues.db.backupBucket }}
  backupNum: {{ .StateValues.db.backupNum }}
  backupSchedule: {{- if hasKey .Values.db "backupSchedule" }}{{ .Values.db.backupSchedule }}{{- else }} '00 00 * * *' {{- end }}
  cloneEnabled: {{- if hasKey .Values.db "cloneEnabled" }} {{ .Values.db.cloneEnabled }} {{- else }} false {{- end }}
  cloneTimeStamp: {{- if hasKey .Values.db "timeStamp" }}{{ .Values.db.timeStamp }}{{- else }} '{{ now | date "2006-01-02T15:04:05+00:00" }}' {{ end }}


scorpio:
  tag: 3.0.0-SNAPSHOT
  externalHostname: {{ .StateValues.scorpio.externalHostname }}
  externalProtocol: {{ .StateValues.scorpio.externalProtocol | quote }}
  externalPath: {{ .StateValues.scorpio.externalPath }}
  internalHostname: scorpio-all-in-one-runner
  internalPort: 9090
  internalProtocol: "http:"
  heap_min:
    Xms: "-Xms64M"
    Xmx: "-Xmx64M"
  heap_main:
    Xms: "-Xms64M"
    Xmx: "-Xmx64M"
  hpa:
    enabled: false
  resources_min:
    limits:
      cpu: 100m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 256Mi
  resources_main:
    limits:
      memory: 512Mi
    requests:
      cpu: 200m
      memory: 256Mi

certmanager:
  secret: {{ .StateValues.certmanager.secret }}
  issuer: {{ .StateValues.certmanager.issuer }}

flink:
  imageTag: 1.16.1
  clusterId: iff
  taskmanagers: 1
  jobmanager: 1
  sqlClientPort: 9000
  bucket: flink
  haDir: recovery
  checkpointDir: checkpoints
  checkpointInterval: 1 min
  savepointDir: savepoints
  defaultParalellism: 1
  alertWindow: '0.001'
  ngsildUpdateWindow: '0.001'
  attributeInsertWindow: '0.001'
  jobmanagerCacheSize: 2Gi
  taskmanagerCacheSize: 10Gi

minio:
  enabled: {{ .StateValues.minio.enabled }}
  storageSize: {{ .StateValues.minio.storageSize }}
  healthPath: /minio/health/live
  adminAccessKey: console
  mcImage: minio/mc:RELEASE.2023-02-28T00-12-59Z


s3:
  {{- $s3 := dict "" }}
  {{- $protocol := "http" }}
  {{- if (hasKey .StateValues "s3") }}{{- $s3 = .StateValues.s3 }}{{- end }}
  protocol: {{ if (hasKey $s3 "protocol") }}{{ quote $s3.protocol }}{{- else }} http {{- end }}
  endpoint: {{ if (hasKey $s3 "endpoint") }}{{ quote $s3.endpoint }}{{- else }} minio.{{ .StateValues.namespace}}.svc.cluster.local {{- end }}
  userAccessKey: {{ if (hasKey $s3 "userAccessKey") }}{{ quote $s3.userAccessKey }}{{- else }} minio {{- end }}
  userSecretKey: {{ if (hasKey $s3 "userSecretKey") }}{{ quote $s3.userSecretKey }}{{- else }} "{{ .StateValues.minioUserSecretKey }}" {{- end }}

alerta:
  externalHostname: {{ .StateValues.alerta.externalHostname }}
  externalPath: {{ .StateValues.alerta.externalPath }}
  adminUser: admin
  internalService: alerta
  internalPort: 8080
  internalProtocol: "http:"
  # apiKey: {{ .StateValues.alertaApiKey }}
  adminPassword: {{ .StateValues.alertaAdminPassword }}
  adminKey: {{ .StateValues.alertaAdminKey }}

kafka:
  name: my-cluster
  bootstrapServer: my-cluster-kafka-bootstrap:9092
  storage:
    type: persistent-claim
    size: 5Gi
  zookeeper:
    replicas: 1
    storage:
      type: persistent-claim
      size: 1Gi
  connect:
    debeziumTopicPrefix: iff.ngsild
    tableIncludeList: public.entity$
    image: debezium-postgresql-connector
    snapshotMode: always

kafkaBridge:
  debezium:
    replicaCount: 1
    listenTopic: "iff.ngsild.public.entity"
    listenTopicRetention: "86400000"
    entityTopicPrefix: "iff.ngsild.entities"
    attributesTopic: "iff.ngsild.attributes"
    entityTopicRetention: "604800000"
    attributesTopicRetention: "86400000"
  alerta:
    replicaCount: 1
    listenTopic: "iff.alerts"
    bulkTopic: "iff.alerts.bulk"
    bulkTopicRetention: "300000"
    listenTopicRetention: 86400000
  ngsildUpdates:
    replicaCount: 1
    tokenRefreshInterval: 200
    listenTopic: "iff.ngsild-updates"
    listenTopicRetention: "86400000"

keycloak:
  adminName: admin
  adminPassword: {{ .StateValues.keycloakpassword }}
  externalAuthService: # put here the *external* keycloak name, i.e. through ingress
    protocol: {{ .StateValues.keycloak.externalAuthService.protocol | quote }}
    domainname: {{ .StateValues.keycloak.externalAuthService.domainname | quote }}
    path: {{ .StateValues.keycloak.externalAuthService.path | quote }}
  internalAuthService: 
    name: keycloak-service
    port: 8080
    path: /auth
  oisp:
    frontendUrl: http://frontend.oisp.svc.cluster.local:4004
    frontend:
      client: oisp-frontend
      clientSecret: {{ .StateValues.oispFrontendClientSecret }}
    mqttBroker:
      client: mqtt-broker
      clientSecret: {{ .StateValues.mqttBrokerClientSecret }}
    fusionBackend:
      client: fusion-backend
      clientSecret: {{ .StateValues.fusionBackendClientSecret }}
    fusionFrontend:
      client: fusion-frontend
  alerta:
    clientSecret: {{ .StateValues.alertaClientSecret }}
    realm: iff
    client: alerta-ui
    redirectUris:
    - http://{{ .StateValues.alerta.externalHostname }}{{ .StateValues.alerta.externalPath }}*
    - https://{{ .StateValues.alerta.externalHostname }}{{ .StateValues.alerta.externalPath }}*
    defaultClientScopes:
    - oisp-frontend
    - accounts
    - offline_access
    - openid
    - profile
    - email
    - type
    - roles
  scorpio:
    realm: iff
    client: scorpio
  ngsildUpdates:
    clientSecret: {{ .StateValues.ngsildUpdatesClientSecret }}
    realm: iff
    client: ngsild-updates
    serviceRole: scorpio.Factory-Admin
  realmTestUser:
    username: "realm_user"
    password: {{ .StateValues.keycloakRealmTestUser }}
keycloak_db:
  stringData:
    POSTGRES_DATABASE: keycloakdb
    POSTGRES_EXTERNAL_ADDRESS: {{ .StateValues.db.teamId -}}-{{- .StateValues.db.clusterSvcPostfix }}
    POSTGRES_ADDR: {{ .StateValues.db.teamId -}}-{{- .StateValues.db.clusterSvcPostfix }}
    POSTGRES_EXTERNAL_PORT: {{ .StateValues.db.svcPort | quote }}
    POSTGRES_SUPERUSER: "true"
    POSTGRES_USERNAME: {{ .StateValues.db.dbUser }}

velero:
  image:
    repository: velero/velero
    tag: v1.10.0
    pullPolicy: IfNotPresent
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      cpu: 200m
      memory: 512Mi
  initContainers:
    - name: velero-plugin-for-aws
      image: velero/velero-plugin-for-aws:v1.5.0
      imagePullPolicy: IfNotPresent
      volumeMounts:
        - mountPath: /target
          name: plugins
  configuration:
    provider: aws
    backupStorageLocation:
      name: minio
      bucket: {{ .StateValues.velero.backupBucket }}
      default: true
      accessMode: ReadWrite
      config:
        region: de
        s3Url: {{ if (hasKey $s3 "protocol") }} {{ $s3.protocol }} {{- else }} http{{- end }}://{{ if (hasKey $s3 "endpoint") }}{{ quote $s3.endpoint }}{{- else }}minio.{{ .StateValues.namespace }}.svc.cluster.local{{- end }}
        s3ForcePathStyle: {{- if .StateValues.minio.enabled }} true {{- else }} false {{- end }}
      defaultVolumesToRestic: true
  credentials:
    useSecret: true
    existingSecret: "velero-s3-credentials"
  # deployNodeAgent: true
  snapshotsEnabled: false
  nodeAgent:
    #podVolumePath: /var/lib/kubelet/pods
    #privileged: false
    # Pod priority class name to use for the node-agent daemonset. Optional.
    #priorityClassName: ""
    # Resource requests/limits to specify for the node-agent daemonset deployment. Optional.
    # https://velero.io/docs/v1.6/customize-installation/#customize-resource-requests-and-limits
    #resources:
    #  requests:
    #    cpu: 500m
    #    memory: 512Mi
    #  limits:
    #    cpu: 1000m
    #    memory: 1024Mi