{{- $secret := "" }}
{{- if .Values.minio.enabled }}
  {{- $secret = (lookup "v1" "Secret" .Release.Namespace "minio-user") -}}
{{- end }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: flink-config
  namespace: {{ .Release.Namespace }}
  labels:
    app: flink
data:
  flink-conf.yaml: |+
    jobmanager.rpc.address: flink-jobmanager
    taskmanager.numberOfTaskSlots: 4
    pipeline.max-parallelism: 4
    blob.server.port: 6124
    jobmanager.rpc.port: 6123
    taskmanager.rpc.port: 6122
    rest.flamegraph.enabled: true

    queryable-state.proxy.ports: 6125
    jobmanager.memory.process.size: 1000m
    taskmanager.memory.process.size: 2500m

    s3.endpoint: {{ printf "%s://%s" .Values.s3.protocol .Values.s3.endpoint }}
    {{- if .Values.minio.enabled }}
    s3.path.style.access: true
    {{- end }}
    s3.access-key: {{ .Values.s3.userAccessKey }}
    {{ if $secret }}
    s3.secret-key: {{ $secret.data.CONSOLE_SECRET_KEY | b64dec }}
    {{ else }}
    s3.secret-key: {{ .Values.s3.userSecretKey }}
    {{ end }}
    state.backend: rocksdb
    state.backend.rocksdb.localdir: /tmp/rocksdb
    state.backend.incremental: false
    state.backend.rocksdb.writebuffer.size: 64 kb
    state.backend.rocksdb.compaction.level.target-file-size-base: 64 kb
    state.backend.rocksdb.use-bloom-filter: true
    state.backend.rocksdb.predefined-options: SPINNING_DISK_OPTIMIZED
    state.checkpoints.dir: s3://{{ .Values.flink.bucket }}/{{ .Values.flink.checkpointDir }}
    state.savepoints.dir: s3://{{ .Values.flink.bucket }}/{{ .Values.flink.savepointDir }}
    kubernetes.cluster-id: {{ .Values.flink.clusterId }}
    high-availability: kubernetes
    high-availability.storageDir: s3://{{ .Values.flink.bucket }}/{{ .Values.flink.haDir }}
    kubernetes.namespace: {{ .Release.Namespace }}
    restart-strategy: exponential-delay
    restart-strategy.exponential-delay.max-backoff: 2 min
    execution.checkpointing.interval: {{ .Values.flink.checkpointInterval }}
    process.working-dir: /tmp/process
  log4j-console.properties: |+
    # This affects logging for both user code and Flink
    rootLogger.level = INFO
    rootLogger.appenderRef.console.ref = ConsoleAppender
    rootLogger.appenderRef.rolling.ref = RollingFileAppender

    # Uncomment this if you want to _only_ change Flink's logging
    #logger.flink.name = org.apache.flink
    #logger.flink.level = TRACE

    # The following lines keep the log level of common libraries/connectors on
    # log level INFO. The root logger does not override this. You have to manually
    # change the log levels here.
    logger.akka.name = akka
    logger.akka.level = INFO
    logger.kafka.name= org.apache.kafka
    logger.kafka.level = INFO
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = INFO
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = INFO

    # Log all infos to the console
    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

    # Log all infos in the given rolling file
    appender.rolling.name = RollingFileAppender
    appender.rolling.type = RollingFile
    appender.rolling.append = false
    appender.rolling.fileName = ${sys:log.file}
    appender.rolling.filePattern = ${sys:log.file}.%i
    appender.rolling.layout.type = PatternLayout
    appender.rolling.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    appender.rolling.policies.type = Policies
    appender.rolling.policies.size.type = SizeBasedTriggeringPolicy
    appender.rolling.policies.size.size=100MB
    appender.rolling.strategy.type = DefaultRolloverStrategy
    appender.rolling.strategy.max = 10

    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name = org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF 